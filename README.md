# Vanishing-Gradient-Problem
Certain activation functions are added to neural networks, the gradients of the loss function approaches zero, making the network hard to train.
